# NLP-Adventures

## Setup
- [optional, only if you are on an LSF-based cluster (e.g. ETHZ Leonhard)] Load python_gpu:<br />`. utils/load_module_cluster.sh`.
- Set up the environment: `. utils/set_env.sh`.
- Install the dependencies: if you only want to do a prediction with the already existing features, run <br />
`make light_install`. If you want a full install that can generate the features, run `make install_gpu`.

## Data structure
The user can choose the location of the working directory (e.g. directly in `task2/`). This working directory should be composed of three subfolders:
- `data/`: data used by the models. It should contain the files `train_stories.csv` (training set of the Story Cloze task), `val_stories.csv` (validation set of the Story Cloze task), `test_stories.csv` (test set of the Story Cloze task) and optionally `eth_nlu18.csv` (special test set of the ETH). For convenience, you can download all the data with this link: https://polybox.ethz.ch/index.php/s/UDs0jon0e3X9Ypm
- `features/`: contains the precomputed features stored as .npz files (each entry in the .npz file should correspond to one feature and be a numpy array of dimension 1).
- `outputs/`: stores the output files generated by the models. This folder is created during the prediction if it doesn't already exist.
- `configs/`: stores the config files. Use `eth_test_config.yaml` to reproduce the produce the results of our final model on the ETH test set. The main parameter to change is `features`: it is a list of the features that you want to use in the model. These features should have the same name in the folders `features/val/`, `features/test/` and `features/eth_test/`.

## Usage

### Generate the features
#### Topic features
Go in `task2/` directory and run `python topics_consistency.py <data_path> <output_path>`.

- `<data_path>`: path to the data file.
- `<output_path>`: path to the output file.

Example:
`python topics_consistency.py data/val_stories.csv features/val/topic.npz`

#### Sentiment features
To recreate the sentiment features, please use the following command from `task2/` directory:
`python sentiment_analysis.py <data_path> <output_path> <config_path> <test_file_name>`

- `<data_path>`: path to the data directory. 
- `<output_path>`: path to the output file. 
- `<config_path>`: path to the config file. Choose: 'sent_vader_config.yaml', 'sent_bl_config.yaml' or 'sent_mpqa_config.yaml'.
Note: to recreate all the features used in the final model, you have to run the 3 different configs sequentially. Please note that the last two can take up to 5 hours on a cluster.
- `<test_file_name>`: Name of the test file. Choose between val_stories.csv, test_stories.csv, or test_nlu18.csv. 
Note: the file names have to have these names exactly, and be in utf-8 format.

Example:
`python sentiment_analysis.py data features/test/sentiment_vader.npz configs/sent_vader_config.yaml test_stories.csv`

#### Textual entailment features
Go in `task2/` directory and run `python textual_entailment.py <data_path> <output_path> --comparisons --conditional --no_header --no_id`.

- `<data_path>`: path to the data file.
- `<output_path>`: path to the output file.
The following arguments are optional:
- `--comparisons`: add the binary features comparing the probabilities of the two endings.
- `--conditional`: add conditional probabilities to the features.
- `--no_header`: if the data file has no header (use this for ETH test set).
- `--no_id`: if the data file has no ID column (use this for ETH test set).

Example:
To reproduce the features for the ETH test set in our final model, run
`python textual_entailment.py data/eth_nlu18.csv features/eth_test --comparisons --conditional --no_header --no_id`


NB: We do not provide the instructions to generate the SemLM features as the setup is quite complicated, with a lot of paths and dependencies issues due to the python-java interface.
These features are not used in the final model and thus do not have an impact on the accuracy of our model. If you would like to know how to create these features or to get the weights of the RNN that we trained, please contact us.

### Train the model given the features and make a prediction
`python experiment.py <base_path> <path_to_config_file> --exp_name <name_of_the_experiment> --test`

- `<base_path>`: path to the working directory described above.
- `<path_to_config_file>`: path to a config file. This config file contains for example the list of features that should be used by the model.
- `--exp_name <name_of_the_experiment>`: optional argument. Name of the experiment which will be given to the output file when a prediction on the test set is made.
- `--test`: use this flag to make a prediction on the test set (not used by default).

Examples:
- Train a logistic regression on the validation set of SCT and compute the accuracy on the SCT test set using all the precomputed features:
`python experiment.py ./ configs/default_config.yaml`.
- Train a logistic regression on the validation set of SCT, compute the accuracy on the SCT test set using only entailment and sentiment features (best model) and make a prediction on the ETH testset:
`python experiment.py ./ configs/eth_test_config.yaml --exp_name eth_test_prediction --test`.
